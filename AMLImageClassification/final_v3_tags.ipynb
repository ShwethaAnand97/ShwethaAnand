{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import imageio\n",
    "import csv\n",
    "\n",
    "descriptions_train_dir = 'data/descriptions_train/'\n",
    "features_train_dir = 'data/features_train/'\n",
    "images_train_dir = 'data/images_train/'\n",
    "tags_train_dir = 'data/tags_train/'\n",
    "\n",
    "descriptions_test_dir = 'data/descriptions_test/'\n",
    "features_test_dir = 'data/features_test/'\n",
    "images_test_dir = 'data/images_test/'\n",
    "tags_test_dir = 'data/tags_test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bowl soup carrot shrimp noodl food bowl soup carrot shrimp chopstick bowl ramen someon bowl asian noodl soup shrimp carrot\n",
      "bowl soup carrot shrimp noodl healthi food bowl readi eat soup carrot shrimp sit next chopstick tasti bowl ramen serv someon enjoy bowl asian noodl soup shrimp carrot\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def parse_description(i, nouns_only):\n",
    "    descriptions_train_file = open(descriptions_train_dir + str(i) + '.txt', 'r') \n",
    "    lines = descriptions_train_file.read()\n",
    "    # print(lines)\n",
    "    \n",
    "    if nouns_only:\n",
    "        nouns = []\n",
    "        for word,pos in nltk.pos_tag(nltk.word_tokenize(lines)):\n",
    "            if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n",
    "                nouns.append(word)\n",
    "    else:\n",
    "        nouns = nltk.word_tokenize(lines)\n",
    "    \n",
    "    words = [w.lower() for w in nouns]\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "    # words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "def parse_tag(i):\n",
    "    tag_train_file = open(tags_train_dir + str(i) + '.txt', 'r') \n",
    "    lines = tag_train_file.read()\n",
    "    if lines == '':\n",
    "        return []\n",
    "    tags_pre = lines.rstrip().split('\\n')\n",
    "    tags_pre = [w.split(':')[1] for w in tags_pre]\n",
    "    \n",
    "#     tags = []\n",
    "#     for w in tags_pre:\n",
    "#         for ww in w.split(' '):\n",
    "#             tags.append(ww)\n",
    "#     tags = [stemmer.stem(w) for w in tags]\n",
    "    return tags_pre\n",
    "    \n",
    "print(parse_description(1, True))\n",
    "print(parse_description(1, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4923\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "training_size = 10000\n",
    "\n",
    "training_descriptions = []\n",
    "training_tags = []\n",
    "tags_set = set()\n",
    "\n",
    "for i in range(training_size):\n",
    "    training_descriptions.append(parse_description(i, True))\n",
    "    training_tags = parse_tag(i)\n",
    "    tags_set |= set(training_tags)\n",
    "    \n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(training_descriptions)\n",
    "X = X.toarray()\n",
    "print( len(vectorizer.get_feature_names()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "{'clock', 'refrigerator', 'suitcase', 'hot dog', 'frisbee', 'chair', 'cell phone', 'oven', 'couch', 'giraffe', 'bear', 'bowl', 'cat', 'stop sign', 'parking meter', 'person', 'train', 'bench', 'bottle', 'bed', 'dog', 'microwave', 'book', 'tennis racket', 'banana', 'toaster', 'teddy bear', 'wine glass', 'bird', 'kite', 'potted plant', 'motorcycle', 'pizza', 'hair drier', 'baseball bat', 'keyboard', 'skis', 'sandwich', 'airplane', 'baseball glove', 'carrot', 'broccoli', 'truck', 'toilet', 'fire hydrant', 'bicycle', 'laptop', 'sheep', 'snowboard', 'car', 'cake', 'umbrella', 'cup', 'scissors', 'tie', 'cow', 'tv', 'knife', 'spoon', 'dining table', 'sink', 'traffic light', 'surfboard', 'sports ball', 'bus', 'apple', 'remote', 'elephant', 'horse', 'mouse', 'handbag', 'vase', 'skateboard', 'toothbrush', 'boat', 'donut', 'fork', 'zebra', 'orange', 'backpack'}\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "training_tags = []\n",
    "tags_set = set()\n",
    "\n",
    "for i in range(training_size):\n",
    "    tags = parse_tag(i)\n",
    "    training_tags.append(tags)\n",
    "    tags_set |= set(tags)\n",
    "print(len(tags_set))\n",
    "print(tags_set)\n",
    "\n",
    "tag2index = {}\n",
    "idx = 0\n",
    "for tag in tags_set:\n",
    "    tag2index[tag] = idx\n",
    "    idx += 1\n",
    "    \n",
    "y = np.zeros((training_size, len(tags_set)))\n",
    "for i in range(training_size):\n",
    "    for tag in training_tags[i]:\n",
    "        y[i][tag2index[tag]] = 1\n",
    "\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get top 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_20_neighbors(target, candid):\n",
    "    index2dist = {}\n",
    "    idx = 0\n",
    "    for c in candid:\n",
    "        d = np.linalg.norm(target - c)\n",
    "        index2dist[idx] = d\n",
    "        idx += 1\n",
    "\n",
    "    sorted_by_value = sorted(index2dist.items(), key=lambda kv: kv[1])\n",
    "    keys = []\n",
    "    for s in sorted_by_value:\n",
    "        keys.append(s[0])\n",
    "    return keys[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "label = [i for i in range(10000)]\n",
    "shuffle(label)\n",
    "\n",
    "X_train = np.zeros((8000, X.shape[1]))\n",
    "y_train = np.zeros((8000, 80))\n",
    "X_test = np.zeros((2000, X.shape[1]))\n",
    "y_test = np.zeros((2000, 80))\n",
    "\n",
    "y_label = []\n",
    "\n",
    "for i in range(8000):\n",
    "    X_train[i] = X[label[i]]\n",
    "    y_train[i] = y[label[i]]\n",
    "for i in range(2000):\n",
    "    X_test[i] = X[label[i + 8000]]\n",
    "    y_test[i] = y[label[i + 8000]]\n",
    "    y_label.append(label[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.60180886  0.14567559 -1.49503461 ...  0.00944589 -0.02258241\n",
      " -0.00586142]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_components = 1024\n",
    "pca = PCA(n_components=pca_components)\n",
    "X_pca = pca.fit_transform(X)\n",
    "print(X_pca[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with full set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "clf = MLPRegressor(solver='sgd', alpha=1e-5, max_iter=2000,\n",
    "                hidden_layer_sizes=(2048, 2048, 1024), random_state=1)\n",
    "clf.fit(X, y)\n",
    "print(\"finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 4923)\n",
      "(10000, 80)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "0.2808999999999999\n"
     ]
    }
   ],
   "source": [
    "y_predict = clf.predict(X_test)\n",
    "\n",
    "idx = 0\n",
    "score_sum = 0\n",
    "tops = []\n",
    "for y_p in y_predict:\n",
    "    top_20 = closest_20_neighbors(y_p, y_test)\n",
    "    tops.append(top_20)\n",
    "    j = 1\n",
    "    for i in top_20:\n",
    "        if i == idx:\n",
    "            # MAP@20\n",
    "            score_sum += float(21 - j) / 20.0\n",
    "            break\n",
    "        j += 1\n",
    "    idx += 1\n",
    "    if idx % 100 == 0:\n",
    "        print(idx)\n",
    "    \n",
    "score = score_sum / 2000.0\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[879, 1568, 9, 104, 198, 329, 497, 634, 828, 1028, 1219, 1244, 1300, 1363, 1537]\n",
      "[1889, 1239, 1451, 1220, 1790, 945, 786, 523, 1543, 1, 359, 1818, 1912, 1986, 820]\n",
      "[950, 1240, 2, 582, 1843, 9, 104, 198, 329, 497, 634, 828, 1028, 1219, 1244]\n",
      "[3, 52, 66, 95, 108, 123, 165, 251, 289, 361, 396, 422, 447, 449, 492]\n",
      "[9, 104, 198, 329, 497, 634, 828, 1028, 1219, 1244, 1300, 1363, 1537, 1538, 1570]\n",
      "[5, 8, 84, 115, 373, 386, 444, 453, 461, 518, 559, 658, 690, 693, 827]\n",
      "[789, 890, 1596, 302, 366, 507, 849, 1416, 782, 1031, 163, 315, 778, 9, 104]\n",
      "[1139, 302, 366, 507, 849, 1416, 1886, 33, 107, 899, 686, 1007, 1761, 144, 1301]\n",
      "[5, 8, 84, 115, 373, 386, 444, 453, 461, 518, 559, 658, 690, 693, 827]\n",
      "[9, 104, 198, 329, 497, 634, 828, 1028, 1219, 1244, 1300, 1363, 1537, 1538, 1570]\n",
      "[1100, 1488, 1858, 189, 440, 882, 1192, 1383, 1454, 1734, 1830, 1839, 1940, 958, 1204]\n",
      "[11, 612, 871, 1849, 214, 9, 104, 198, 329, 497, 634, 828, 1028, 1219, 1244]\n",
      "[1692, 445, 574, 740, 1330, 1837, 1991, 1523, 1842, 1877, 577, 543, 907, 1209, 586]\n",
      "[879, 9, 104, 198, 329, 497, 634, 828, 1028, 1219, 1244, 1300, 1363, 1537, 1538]\n",
      "[1220, 1889, 1451, 945, 1388, 1462, 1986, 1239, 1790, 97, 1818, 820, 523, 1, 1543]\n",
      "[15, 202, 290, 334, 403, 429, 526, 681, 692, 708, 709, 717, 803, 999, 1017]\n",
      "[703, 768, 769, 1182, 1827, 302, 366, 507, 849, 1416, 16, 93, 590, 1605, 1648]\n",
      "[43, 156, 174, 398, 579, 745, 837, 847, 876, 916, 951, 1032, 1143, 1261, 1278]\n",
      "[18, 1483, 238, 730, 1519, 1497, 1641, 9, 104, 198, 329, 497, 634, 828, 1028]\n",
      "[1497, 1641, 9, 104, 198, 329, 497, 634, 828, 1028, 1219, 1244, 1300, 1363, 1537]\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(tops[i][:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4923\n"
     ]
    }
   ],
   "source": [
    "word2index = {}\n",
    "\n",
    "idx = 0\n",
    "for w in vectorizer.get_feature_names():\n",
    "    word2index[w] = idx\n",
    "    idx += 1\n",
    "\n",
    "print(len(word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_test_dir = 'data/descriptions_test/'\n",
    "features_test_dir = 'data/features_test/'\n",
    "images_test_dir = 'data/images_test/'\n",
    "tags_test_dir = 'data/tags_test/'\n",
    "\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def parse_test_description(i):\n",
    "    descriptions_test_file = open(descriptions_test_dir + str(i) + '.txt', 'r') \n",
    "    lines = descriptions_test_file.read()\n",
    "    # print(lines)\n",
    "    \n",
    "    nouns = []\n",
    "    for word,pos in nltk.pos_tag(nltk.word_tokenize(lines)):\n",
    "        if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n",
    "            nouns.append(word)\n",
    "    \n",
    "    words = [w.lower() for w in nouns]\n",
    "    words = [w for w in words if not w in stop_words] # strip stopword\n",
    "    words = [stemmer.stem(w) for w in words] # stem\n",
    "    \n",
    "    return words\n",
    "    \n",
    "def parse_test_tag(i):\n",
    "    tag_test_file = open(tags_test_dir + str(i) + '.txt', 'r') \n",
    "    lines = tag_test_file.read()\n",
    "    if lines == '':\n",
    "        return []\n",
    "    tags_pre = lines.rstrip().split('\\n')\n",
    "    tags_pre = [w.split(':')[1] for w in tags_pre]\n",
    "\n",
    "    return tags_pre\n",
    "\n",
    "test_size = 2000\n",
    "X_real_test = np.zeros((test_size, len(vectorizer.get_feature_names())))\n",
    "\n",
    "for i in range(test_size):\n",
    "    words = parse_test_description(i)\n",
    "    for w in words:\n",
    "        if w in word2index:\n",
    "            X_real_test[i][word2index[w]] += 1    \n",
    "\n",
    "# X_real_test_pca = pca.transform(X_real_test)\n",
    "\n",
    "\n",
    "test_tags = []\n",
    "for i in range(test_size):\n",
    "    tags = parse_test_tag(i)\n",
    "    test_tags.append(tags)\n",
    "\n",
    "y_real_test = np.zeros((test_size, len(tags_set)))\n",
    "for i in range(test_size):\n",
    "    for tag in test_tags[i]:\n",
    "        y_real_test[i][tag2index[tag]] = 1\n",
    "\n",
    "y_real_test_predict = clf.predict(X_real_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "res = []\n",
    "for y_p in y_real_test_predict:\n",
    "    top_20 = closest_20_neighbors(y_p, y_real_test)\n",
    "    res.append(top_20)\n",
    "    idx += 1\n",
    "    if idx % 100 == 0:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = 'submission_tags_mlp.csv'\n",
    "with open(output_filename, 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Descritpion_ID\",\"Top_20_Image_IDs\"])\n",
    "    for i in range(test_size):\n",
    "        row = []\n",
    "        row.append(str(i) + '.txt')\n",
    "        candids = []\n",
    "        for candid in res[i]:\n",
    "            candids.append(str(candid) + '.jpg')\n",
    "        row.append(' '.join(candids))\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
