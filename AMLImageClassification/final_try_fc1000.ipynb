{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2049\n",
      "2049\n",
      "2049\n",
      "2049\n",
      "2049\n",
      "2049\n"
     ]
    }
   ],
   "source": [
    "# prepare libraries & data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import imageio\n",
    "import csv\n",
    "\n",
    "descriptions_train_dir = 'data/descriptions_train/'\n",
    "features_train_dir = 'data/features_train/'\n",
    "images_train_dir = 'data/images_train/'\n",
    "tags_train_dir = 'data/tags_train/'\n",
    "\n",
    "def read_example(n):\n",
    "    print('Descriptions:')\n",
    "    descriptions_train_file = open(descriptions_train_dir + str(n) + '.txt', 'r') \n",
    "    print(descriptions_train_file.read())\n",
    "    \n",
    "    print('Tags:')\n",
    "    tags_train_file = open(tags_train_dir + str(n) + '.txt', 'r') \n",
    "    print(tags_train_file.read())\n",
    "    \n",
    "    im = imageio.imread(images_train_dir + str(n) + '.jpg')\n",
    "    plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.03315091, 0.14875449, 0.04642047, ..., 0.07316806, 0.41725951,\n",
       "        0.19742125],\n",
       "       [0.23184368, 0.12003349, 0.09078766, ..., 0.27578694, 3.49657249,\n",
       "        0.06475818],\n",
       "       [0.62282479, 0.25091803, 0.68479973, ..., 0.46948081, 0.70399392,\n",
       "        0.23659612],\n",
       "       ...,\n",
       "       [0.05613672, 1.93112803, 0.23134135, ..., 0.36287165, 0.12175643,\n",
       "        0.31296232],\n",
       "       [1.04675007, 0.54812443, 0.48379216, ..., 0.17267197, 0.93013376,\n",
       "        0.42781889],\n",
       "       [0.16653799, 0.85952431, 0.16225892, ..., 0.32290319, 0.54848814,\n",
       "        0.17039269]])"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_resnet1000intermediate_train = np.zeros((10000, 2048)) # resnet feature\n",
    "\n",
    "filename = 'features_resnet1000intermediate_train.csv'\n",
    "with open(features_train_dir + filename) as f:\n",
    "    reader = csv.reader(f)\n",
    "    for line in reader:\n",
    "        index = int(line[0][13:].split('.')[0])\n",
    "        features_resnet1000intermediate_train[index] = line[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_test_dir = 'data/descriptions_test/'\n",
    "features_test_dir = 'data/features_test/'\n",
    "images_test_dir = 'data/images_test/'\n",
    "tags_test_dir = 'data/tags_test/'\n",
    "\n",
    "def read_test_example(n):\n",
    "    print('Descriptions:')\n",
    "    descriptions_test_file = open(descriptions_test_dir + str(n) + '.txt', 'r') \n",
    "    print(descriptions_test_file.read())\n",
    "    \n",
    "    print('Tags:')\n",
    "    tags_test_file = open(tags_test_dir + str(n) + '.txt', 'r') \n",
    "    print(tags_test_file.read())\n",
    "    \n",
    "    im = imageio.imread(images_test_dir + str(n) + '.jpg')\n",
    "    plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bowl soup carrot shrimp noodl food bowl soup carrot shrimp chopstick bowl ramen someon bowl asian noodl soup shrimp carrot'"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def parse_description(i):\n",
    "    descriptions_train_file = open(descriptions_train_dir + str(i) + '.txt', 'r') \n",
    "    lines = descriptions_train_file.read()\n",
    "    # print(lines)\n",
    "    \n",
    "    nouns = []\n",
    "    for word,pos in nltk.pos_tag(nltk.word_tokenize(lines)):\n",
    "        if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n",
    "            nouns.append(word)\n",
    "            \n",
    "    words = [w.lower() for w in nouns]\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "    # words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    \n",
    "    # TODO remain nouns\n",
    "\n",
    "    return ' '.join(words)\n",
    "    \n",
    "parse_description(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "training_size = 10000\n",
    "\n",
    "training_descriptions = []\n",
    "for i in range(training_size):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    training_descriptions.append(parse_description(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4923\n",
      "['abdomen', 'abil', 'abll', 'aboard', 'abook', 'abou', 'abraham', 'abreast', 'abund', 'abyss', 'acacia', 'accent', 'access', 'accessori', 'accid', 'account', 'ace', 'acket', 'aclock', 'acm', 'acorn', 'across', 'act', 'action', 'activ', 'actor', 'ad', 'add', 'addit', 'address', 'adjac', 'adjust', 'adolesc', 'adult', 'adventur', 'advertis', 'aer', 'aeroplan', 'afar', 'affair', 'affect', 'afghan', 'afield', 'africa', 'afro', 'afterburn', 'afternoon', 'agaist', 'age', 'agenc']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(training_descriptions)\n",
    "X = X.toarray()\n",
    "print( len(vectorizer.get_feature_names()) )\n",
    "print( vectorizer.get_feature_names()[0:50] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "def mlp(X, y, test):\n",
    "    clf = MLPRegressor(solver='sgd', alpha=1e-5, max_iter=2000,\n",
    "                    hidden_layer_sizes=(2048, 1024, 1024), random_state=1)\n",
    "    clf.fit(X, y)\n",
    "    return clf.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get top 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def closest_20_neighbors(target, candid):\n",
    "    dist2index = {}\n",
    "    idx = 0\n",
    "    \n",
    "    pq = []\n",
    "    for c in candid:\n",
    "        d = np.linalg.norm(target - c)\n",
    "        dist2index[d] = idx\n",
    "        pq.append(d)\n",
    "        idx += 1\n",
    "\n",
    "    res = []\n",
    "    for d in heapq.nsmallest(20, pq):\n",
    "        res.append(dist2index[d])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "label = [i for i in range(10000)]\n",
    "shuffle(label)\n",
    "\n",
    "y = features_resnet1000intermediate_train\n",
    "\n",
    "# wordvec_train = np.zeros((8000, len(vectorizer.get_feature_names())))\n",
    "wordvec_train = np.zeros((8000, pca_components))\n",
    "feature_train = np.zeros((8000, 2048))\n",
    "# wordvec_test = np.zeros((2000, len(vectorizer.get_feature_names())))\n",
    "wordvec_test = np.zeros((2000, pca_components))\n",
    "feature_test = np.zeros((2000, 2048))\n",
    "\n",
    "y_label = []\n",
    "\n",
    "for i in range(8000):\n",
    "    wordvec_train[i] = X_pca[label[i]]\n",
    "    feature_train[i] = y[label[i]]\n",
    "for i in range(2000):\n",
    "    wordvec_test[i] = X_pca[label[i + 8000]]\n",
    "    feature_test[i] = y[label[i + 8000]]\n",
    "    y_label.append(label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict finished\n"
     ]
    }
   ],
   "source": [
    "feature_predict = mlp(wordvec_train, feature_train, wordvec_test)\n",
    "print('predict finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-239-aa140c4da485>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mscore_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my_p\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_predict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtop_20\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosest_20_neighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-151-939c1337f3ab>\u001b[0m in \u001b[0;36mclosest_20_neighbors\u001b[0;34m(target, candid)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mdist2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mpq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "score_sum = 0\n",
    "for y_p in feature_predict:\n",
    "    top_20 = closest_20_neighbors(y_p, feature_test)\n",
    "    j = 1\n",
    "    for i in top_20:\n",
    "        if i == idx:\n",
    "            # MAP@20\n",
    "            score_sum += float(21 - j) / 20.0\n",
    "            break\n",
    "        j += 1\n",
    "    idx += 1\n",
    "    if idx % 100 == 0:\n",
    "        print(idx)\n",
    "score = score_sum / 2000.0\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47319756 0.4687879  0.38619005 ... 0.35677502 0.77253476 0.55194069]\n"
     ]
    }
   ],
   "source": [
    "print(feature_predict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2048)\n",
      "(10000, 4923)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.60180886  0.14567559 -1.49503461 ...  0.00944589 -0.02258241\n",
      " -0.00586142]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_components = 1024\n",
    "pca = PCA(n_components=pca_components)\n",
    "X_pca = pca.fit_transform(X)\n",
    "print(X_pca[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with full set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished.\n"
     ]
    }
   ],
   "source": [
    "clf = MLPRegressor(solver='sgd', alpha=1e-5, max_iter=2000,\n",
    "                hidden_layer_sizes=(4096, 4096, 2048), random_state=1)\n",
    "clf.fit(y, X_pca)\n",
    "print(\"finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "0.7776750000000024\n"
     ]
    }
   ],
   "source": [
    "wordvec_predict = clf.predict(feature_test)\n",
    "\n",
    "idx = 0\n",
    "score_sum = 0\n",
    "tops = []\n",
    "for x_p in wordvec_test:\n",
    "    top_20 = closest_20_neighbors(x_p, wordvec_predict)\n",
    "    tops.append(top_20)\n",
    "    j = 1\n",
    "    for i in top_20:\n",
    "        if i == idx:\n",
    "            # MAP@20\n",
    "            score_sum += float(21 - j) / 20.0\n",
    "            break\n",
    "        j += 1\n",
    "    idx += 1\n",
    "    if idx % 100 == 0:\n",
    "        print(idx)\n",
    "    \n",
    "score = score_sum / 2000.0\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1471, 1848, 1144, 1199, 1775, 658, 1043, 178, 464, 1929]\n",
      "[1, 1713, 765, 1667, 1588, 1733, 1195, 1757, 898, 1819]\n",
      "[678, 1130, 1900, 218, 1860, 1249, 2, 681, 75, 1680]\n",
      "[1158, 933, 1408, 1653, 1604, 990, 988, 306, 971, 1880]\n",
      "[4, 307, 465, 867, 1964, 1020, 1016, 878, 210, 855]\n",
      "[568, 5, 1130, 678, 846, 1860, 1900, 681, 1586, 886]\n",
      "[6, 1872, 856, 631, 627, 1368, 574, 356, 411, 18]\n",
      "[931, 7, 1287, 743, 1929, 204, 1775, 1069, 464, 977]\n",
      "[8, 283, 282, 80, 813, 491, 907, 119, 1690, 875]\n",
      "[9, 514, 621, 1311, 670, 256, 1663, 1562, 481, 360]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(tops[i][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4923\n"
     ]
    }
   ],
   "source": [
    "word2index = {}\n",
    "\n",
    "idx = 0\n",
    "for w in vectorizer.get_feature_names():\n",
    "    word2index[w] = idx\n",
    "    idx += 1\n",
    "\n",
    "print(len(word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_test_dir = 'data/descriptions_test/'\n",
    "features_test_dir = 'data/features_test/'\n",
    "images_test_dir = 'data/images_test/'\n",
    "tags_test_dir = 'data/tags_test/'\n",
    "\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def parse_test_description(i):\n",
    "    descriptions_test_file = open(descriptions_test_dir + str(i) + '.txt', 'r') \n",
    "    lines = descriptions_test_file.read()\n",
    "    # print(lines)\n",
    "    \n",
    "    nouns = []\n",
    "    for word,pos in nltk.pos_tag(nltk.word_tokenize(lines)):\n",
    "        if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n",
    "            nouns.append(word)\n",
    "    \n",
    "    words = [w.lower() for w in nouns]\n",
    "    words = [w for w in words if not w in stop_words] # strip stopword\n",
    "    words = [stemmer.stem(w) for w in words] # stem\n",
    "    \n",
    "    return words\n",
    "    \n",
    "test_size = 2000\n",
    "X_real_test = np.zeros((test_size, len(vectorizer.get_feature_names())))\n",
    "\n",
    "for i in range(test_size):\n",
    "    words = parse_test_description(i)\n",
    "    for w in words:\n",
    "        if w in word2index:\n",
    "            X_real_test[i][word2index[w]] += 1    \n",
    "\n",
    "X_real_test_pca = pca.transform(X_real_test)\n",
    "\n",
    "feature_real_test = np.zeros((2000, 2048)) # resnet feature\n",
    "\n",
    "features_test_dir = 'data/features_test/'\n",
    "filename = 'features_resnet1000intermediate_test.csv'\n",
    "\n",
    "with open(features_test_dir + filename) as f:\n",
    "    reader = csv.reader(f)\n",
    "    for line in reader:\n",
    "        index = int(line[0][12:].split('.')[0])\n",
    "        feature_real_test[index] = line[1:]\n",
    "\n",
    "feature_real_test.astype(float)\n",
    "\n",
    "wordvec_real_test_predict = clf.predict(feature_real_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "res = []\n",
    "for x_p in X_real_test_pca:\n",
    "    top_20 = closest_20_neighbors(x_p, wordvec_real_test_predict)\n",
    "    res.append(top_20)\n",
    "    idx += 1\n",
    "    if idx % 100 == 0:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplan', 'traffic', 'light']\n"
     ]
    }
   ],
   "source": [
    "# Using tags\n",
    "\n",
    "def parse_tag(i):\n",
    "    tag_test_file = open(tags_test_dir + str(i) + '.txt', 'r') \n",
    "    lines = tag_test_file.read()\n",
    "    if lines == '':\n",
    "        return ['?']\n",
    "    tags_pre = lines.rstrip().split('\\n')\n",
    "    tags_pre = [w.split(':')[1] for w in tags_pre]\n",
    "    \n",
    "    tags = []\n",
    "    for w in tags_pre:\n",
    "        for ww in w.split(' '):\n",
    "            tags.append(ww)\n",
    "    tags = [stemmer.stem(w) for w in tags] # stem\n",
    "    return tags\n",
    "\n",
    "test_tags = []\n",
    "for i in range(test_size):\n",
    "    test_tags.append(parse_tag(i))\n",
    "\n",
    "print(test_tags[598])\n",
    "\n",
    "for i in range(test_size):\n",
    "    rank = 1\n",
    "    scores = []\n",
    "    for candid in res[i]:\n",
    "        # compute tags score for each candidate\n",
    "        s = 0\n",
    "        tags = test_tags[candid]\n",
    "        for tag in tags:\n",
    "            if tag in word2index:\n",
    "                s -= X_real_test[i][word2index[tag]] * 5\n",
    "        s += rank\n",
    "        scores.append(s)\n",
    "        rank += 1\n",
    "        \n",
    "    top_idx = sorted(range(20), key=lambda i: scores[i])\n",
    "    new_res = []\n",
    "    for idx in top_idx:\n",
    "        new_res.append(res[i][idx])\n",
    "    res[i] = new_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = 'submission_feature2wordvec_nouns_without_tags_origin.csv'\n",
    "with open(output_filename, 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Descritpion_ID\",\"Top_20_Image_IDs\"])\n",
    "    for i in range(test_size):\n",
    "        row = []\n",
    "        row.append(str(i) + '.txt')\n",
    "        candids = []\n",
    "        for candid in res[i]:\n",
    "            candids.append(str(candid) + '.jpg')\n",
    "        row.append(' '.join(candids))\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
